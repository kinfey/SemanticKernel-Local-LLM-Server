# Local LLM Server for Semantic Kernel .NET Developer

*å¦‚æœä½ ä½¿ç”¨ä¸­æ–‡ï¼Œè¯·[è®¿é—®è¿™é‡Œ](README.zh-cn.md)*

*Support Semantic-Kernel RC.3*

Or you are using Semantic Kernel's [Hugging Face http server](https://github.com/microsoft/semantic-kernel/tree/3451a4ebbc9db0d049f48804c12791c681a326cb/samples/apps/hugging-face-http-server) as your local LLM service  , but based on the inaccessibility of hugging face in mainland China and management reasons, I tried to reconstruct the project. At this stage, it is adapted for macOS and Linux environments.

At this stage, the implementation of ChatCompletion and Embedding has been completed.

**ChatCompletion** is adapted to  LLM  *baichu2*, *ChatGLM3*, *Microsoft-phi 1.5*

**Embeddings** adapted to LLM *jina-embeddings* (English Embedding model), *text2vec-large-chinese* (Chiense Embedding model)

**Samples**


0. download your LLM firstly and using pip to install python library


```bash

pip install -r requirement.txt

```

1. .env config your ChatCompletion and Embedding model location

```txt

CHAT_COMPLETION_URL = 'Your chat completion model location'
EMBEDDING_URL = 'Your embeddings model location'

```

2. Start your Local LLM Http Server

```bash

python local_llm_service.py

```

3. Add Microsoft.SemanticKernel, Microsoft.SemanticKernel.Connectors.AI.HuggingFace, Microsoft.SemanticKernel.Connectors.Memory.Qdrant(You can choose different vector database) packages 

4. Initialization endpoint for chatcompletion, embeddings, and qdrant 


```csharp

string chat_endpoint = "http://localhost:5002/v1/chat/completions";
string embeddings_endpoint = "http://localhost:5002/v1/embeddings";
string qdrant_endpoint = "http://localhost:6333";


```


5. Sample 1 - ChatCompletion


```csharp

using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.Memory.Qdrant;
using Microsoft.SemanticKernel.Plugins.Memory;
using Microsoft.SemanticKernel.Connectors.AI.HuggingFace.TextEmbedding;

#pragma warning disable SKEXP0020

Kernel kernel = new KernelBuilder()
            .AddHuggingFaceTextGeneration(
                model: "baichuan2",
                endpoint: chat_endpoint)
            .Build();

var questionAnswerFunction = kernel.CreateFunctionFromPrompt("é—®: {{$input}} ç­”:");

var result = await kernel.InvokeAsync(questionAnswerFunction, new("ä»‹ç»ä¸€ä¸‹è‡ªå·±"));

result.GetValue<string>()


```

6. Sample 2 - Embeddings


```csharp

#pragma warning disable SKEXP0052
#pragma warning disable CS1061
#pragma warning disable SKEXP0011
#pragma warning disable SKEXP0026

#pragma warning disable SKEXP0020

var qdrantMemoryBuilder = new MemoryBuilder();

var hfembeddings = new HuggingFaceTextEmbeddingGeneration("text2veccn", embeddings_endpoint);

qdrantMemoryBuilder.WithTextEmbeddingGeneration(hfembeddings);
qdrantMemoryBuilder.WithQdrantMemoryStore(qdrant_endpoint, 1024);

var builder = qdrantMemoryBuilder.Build();

string MemoryCollectionName = "text2vecdemo";


await builder.SaveInformationAsync(MemoryCollectionName, id: "id1", text: "æˆ‘æ˜¯å¢å»ºæ™–");
await builder.SaveInformationAsync(MemoryCollectionName, id: "id2", text: "å¢å»ºæ™–æ˜¯å¾®è½¯äº‘æŠ€æœ¯å¸ƒé“å¸ˆ");
await builder.SaveInformationAsync(MemoryCollectionName, id: "id3", text: "å¢å»ºæ™–ä» 2012 å¹´åˆ° 2020 å¹´æ˜¯å¾®è½¯æœ€æœ‰ä»·å€¼ä¸“å®¶");
await builder.SaveInformationAsync(MemoryCollectionName, id: "id4", text: "å¢å»ºæ™–æ˜¯äººå·¥æ™ºèƒ½è®²å¸ˆ");

var searchResults =  builder.SearchAsync(MemoryCollectionName, "ä½ è®¤è¯†å¢å»ºæ™–å—", limit: 3, minRelevanceScore: 0.6);

await foreach (var item in searchResults)
{
    Console.WriteLine(item.Metadata.Text + " : " + item.Relevance);
}


```

If you want to get English text embedding and chat completion , please click [here](./samples/dotnet_notebook_en.ipynb)

ğŸ”ğŸ”ğŸ”ğŸ”ğŸ”ğŸ”ğŸ”ğŸ”ğŸ” More functions will be added in the future 






